{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from jinja2 import Template\n",
    "from PIL import Image\n",
    "from PIL.Image import Image as ImageObject\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer, ProcessorMixin\n",
    "\n",
    "\n",
    "def collate_fn(features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    tensors = defaultdict(list)\n",
    "    non_tensors = defaultdict(list)\n",
    "    for feature in features:\n",
    "        for key, value in feature.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                tensors[key].append(value)\n",
    "            else:\n",
    "                non_tensors[key].append(value)\n",
    "\n",
    "    for key, value in tensors.items():\n",
    "        tensors[key] = torch.stack(value, dim=0)\n",
    "\n",
    "    for key, value in non_tensors.items():\n",
    "        non_tensors[key] = np.array(value, dtype=object)\n",
    "\n",
    "    return {**tensors, **non_tensors}\n",
    "\n",
    "\n",
    "class ImageProcessMixin:\n",
    "    max_pixels: int\n",
    "    min_pixels: int\n",
    "\n",
    "    def process_image(self, image: Union[Dict[str, Any], ImageObject]) -> ImageObject:\n",
    "        if isinstance(image, dict):\n",
    "            image = Image.open(BytesIO(image[\"bytes\"]))\n",
    "        elif isinstance(image, bytes):\n",
    "            image = Image.open(BytesIO(image))\n",
    "\n",
    "        if (image.width * image.height) > self.max_pixels:\n",
    "            resize_factor = math.sqrt(self.max_pixels / (image.width * image.height))\n",
    "            width, height = int(image.width * resize_factor), int(image.height * resize_factor)\n",
    "            image = image.resize((width, height))\n",
    "\n",
    "        if (image.width * image.height) < self.min_pixels:\n",
    "            resize_factor = math.sqrt(self.min_pixels / (image.width * image.height))\n",
    "            width, height = int(image.width * resize_factor), int(image.height * resize_factor)\n",
    "            image = image.resize((width, height))\n",
    "\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class RLHFDataset(Dataset, ImageProcessMixin):\n",
    "    \"\"\"\n",
    "    We assume the dataset contains a column that contains prompts and other information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        processor: Optional[ProcessorMixin],\n",
    "        prompt_key: str = \"prompt\",\n",
    "        answer_key: str = \"answer\",\n",
    "        image_key: str = \"images\",\n",
    "        max_prompt_length: int = 1024,\n",
    "        truncation: str = \"error\",\n",
    "        format_prompt: Optional[str] = None,\n",
    "        max_pixels: Optional[int] = None,\n",
    "        min_pixels: Optional[int] = None,\n",
    "        filter_overlong_prompts: bool = True,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.prompt_key = prompt_key\n",
    "        self.answer_key = answer_key\n",
    "        self.image_key = image_key\n",
    "        self.max_prompt_length = max_prompt_length\n",
    "        self.truncation = truncation\n",
    "        self.max_pixels = max_pixels\n",
    "        self.min_pixels = min_pixels\n",
    "        self.filter_overlong_prompts = filter_overlong_prompts\n",
    "\n",
    "        if \"@\" in data_path:\n",
    "            data_path, data_split = data_path.split(\"@\")\n",
    "        else:\n",
    "            data_split = \"train\"\n",
    "\n",
    "        if os.path.isdir(data_path):\n",
    "            # when we use dataset builder, we should always refer to the train split\n",
    "            self.dataset = load_dataset(\"parquet\", data_dir=data_path, split=\"train\")\n",
    "        elif os.path.isfile(data_path):\n",
    "            self.dataset = load_dataset(\"parquet\", data_files=data_path, split=\"train\")\n",
    "        else:\n",
    "            # load remote dataset from huggingface hub\n",
    "            self.dataset = load_dataset(data_path, split=data_split)\n",
    "\n",
    "        self.format_prompt = None\n",
    "        if format_prompt:\n",
    "            with open(format_prompt, encoding=\"utf-8\") as f:\n",
    "                self.format_prompt = f.read()\n",
    "\n",
    "        if self.filter_overlong_prompts:\n",
    "            self.dataset = self.dataset.filter(self._filter_overlong_prompts, desc=\"Filtering overlong prompts\")\n",
    "\n",
    "    def _build_messages(self, example: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        prompt_str: str = example[self.prompt_key]\n",
    "        \n",
    "        # zhh modified\n",
    "        if self.image_key in example:\n",
    "            if '<image>' not in prompt_str:\n",
    "                prompt_str = \"<image>\" + prompt_str\n",
    "                \n",
    "        if self.format_prompt:\n",
    "            format_prompt = Template(self.format_prompt.strip())\n",
    "            prompt_str = format_prompt.render(content=prompt_str)\n",
    "\n",
    "        if self.image_key in example:\n",
    "            # https://huggingface.co/docs/transformers/en/tasks/image_text_to_text\n",
    "            content_list = []\n",
    "            for i, content in enumerate(prompt_str.split(\"<image>\")):\n",
    "                if i != 0:\n",
    "                    content_list.append({\"type\": \"image\"})\n",
    "\n",
    "                if content:\n",
    "                    content_list.append({\"type\": \"text\", \"text\": content})\n",
    "\n",
    "            return [{\"role\": \"user\", \"content\": content_list}]\n",
    "        else:\n",
    "            return [{\"role\": \"user\", \"content\": prompt_str}]\n",
    "\n",
    "    def _filter_overlong_prompts(self, example: Dict[str, Any]) -> bool:\n",
    "        messages = self._build_messages(example)\n",
    "        processing_class = self.processor if self.processor is not None else self.tokenizer\n",
    "        return (\n",
    "            len(processing_class.apply_chat_template(messages, add_generation_prompt=True)) <= self.max_prompt_length\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example: dict = self.dataset[index]\n",
    "        messages = self._build_messages(example)\n",
    "\n",
    "        if self.image_key in example:\n",
    "            prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "            # zhh modified\n",
    "            image_data = example.pop(self.image_key)\n",
    "            if not isinstance(image_data, list):\n",
    "                image_data = [image_data]\n",
    "            images = [self.process_image(image) for image in image_data]\n",
    "            # images = [self.process_image(image) for image in example.pop(self.image_key)]\n",
    "            model_inputs = self.processor(images, [prompt], add_special_tokens=False, return_tensors=\"pt\")\n",
    "            input_ids = model_inputs.pop(\"input_ids\")[0]\n",
    "            attention_mask = model_inputs.pop(\"attention_mask\")[0]\n",
    "            example[\"multi_modal_data\"] = {\"image\": images}\n",
    "            example[\"multi_modal_inputs\"] = dict(model_inputs)\n",
    "        else:\n",
    "            prompt = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "            model_inputs = self.tokenizer([prompt], add_special_tokens=False, return_tensors=\"pt\")\n",
    "            input_ids = model_inputs.pop(\"input_ids\")[0]\n",
    "            attention_mask = model_inputs.pop(\"attention_mask\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='/home/esg8sdce/esg8sdceuser01/project/mllm_cl/cl_datasets/ScienceQA/test/test-00000-of-00001.parquet@test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/scratch/esg8sdce/pre_trained_models/Qwen2.5-VL-7B-Instruct\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str='<image> What is the chemical formula of water?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/esg8sdce/esg8sdceuser01/mllm_grpo/EasyR1/examples/format_prompt/math_format.jinja'\n",
    "with open(path, encoding=\"utf-8\") as f:\n",
    "    format_prompt = f.read()\n",
    "format_prompt = Template(format_prompt.strip())\n",
    "prompt_str = format_prompt.render(content=prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<image> What is the chemical formula of water? You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\boxed{}.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_str\n",
    "tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt_str}], add_generation_prompt=True, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLProcessor\n",
    "processor = Qwen2VLProcessor.from_pretrained(\"/scratch/esg8sdce/pre_trained_models/Qwen2.5-VL-7B-Instruct\")\n",
    "text = processor.apply_chat_template([{\"role\": \"user\", \"content\": prompt_str}], add_generation_prompt=True, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<image> What is the chemical formula of water? You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\boxed{}.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esg8sdce/esg8sdceuser01/miniconda3/envs/mllm_cl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:42<00:00,  8.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"/scratch/esg8sdce/pre_trained_models/Qwen2.5-VL-7B-Instruct\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"/scratch/esg8sdce/pre_trained_models/Qwen2.5-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2_5_VLForConditionalGeneration(\n",
      "  (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
      "    (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "    )\n",
      "    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
      "        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (attn): Qwen2_5_VLVisionSdpaAttention(\n",
      "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (mlp): Qwen2_5_VLMLP(\n",
      "          (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "          (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "          (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (merger): Qwen2_5_VLPatchMerger(\n",
      "      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model): Qwen2_5_VLModel(\n",
      "    (embed_tokens): Embedding(152064, 3584)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
      "        (self_attn): Qwen2_5_VLSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "          (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm_cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
